[
  {
    "objectID": "quality.html",
    "href": "quality.html",
    "title": "Quality, Validation & Maintenance",
    "section": "",
    "text": "Since correctness and reproducibility is essential to all data processing, validation is a top priority and part of the design and implementation throughout the future ecosystem. Several types of testing are performed.\nFirst, all the essential core packages part of the future framework, future, parallelly, globals, and listenv, implement a rich set of package tests. These are validated regularly across the wide-range of operating systems (Linux, macOS, and MS Windows) and R versions available on CRAN, on continuous integration (CI) services (GitHub Actions), an on R-hub. The core packages are also tested with non-English locale settings, including Korean, Simplified Chinese (China) and Traditional Chinese (Taiwan). They are also verified to work with single-core and dual-core machines, which may be the case for some free, cloud services that provide R.\nSecond, for each new release, these packages undergo full reverse-package dependency checks using revdepcheck. As of July 2025, the future package is tested against 480 direct reverse-package dependencies available on CRAN and Bioconductor. These checks are performed on Linux with both the default settings and when forcing tests to use multisession workers (SOCK clusters), which further validates that globals and packages are identified correctly. We also test with NOT_CRAN = true, to further increase the test coverage.\n\n Third, a suite of Future API conformance tests available in the future.tests package validates the correctness of all future backends. Any new future backend developed, must pass these tests to comply with the Future API. By conforming to this API, the end-user can trust that the backend will produce the same correct and reproducible results as any other backend, including the ones that the backend developer have tested on. Also, by making it the responsibility of the backend developer to assert that their new future backend conforms to the Future API, we relieve other developers from having to test that their future-based software works on all backends. It would be a daunting task for a developer to validate the correctness of their software with all existing backends. Even if they would achieve that, there may be additional third-party future backends that they are not aware of, that they do not have the possibility to test with, or that yet have not been developed.\n\nFourth, since foreach is used by a large number of essential CRAN packages, it provides an excellent opportunity for supplementary validation. Specifically, we dynamically tweak the examples of foreach and popular CRAN packages caret, glmnet, NMF, plyr, and TSP to use the doFuture adaptor. This allows us to run these examples with a variety of future backends to validate that the examples produce no run-time errors, which indirectly validates the backends as well as the Future API. In the past, these types of tests helped to identify and resolve corner cases where automatic identification of global variables would fail. As a side note, several of these foreach-based examples fail when using a parallel foreach adaptor because they do not properly export globals or declare package dependencies. The exception is when using the sequential doSEQ adaptor (default), fork-based ones such as doMC, or the generic doFuture, which supports any future backend and relies on the future framework for handling globals and packages.\nAnalogously to above reverse-dependency checks of each new release, CRAN and Bioconductor continuously run checks on all these direct, but also indirect, reverse dependencies, which further increases the validation of the Future API and the future ecosystem at large.\nThe packages underlying the future ecosystem are well maintained and are continuously updated and improved. For example, the future package is updated approximately six times per year, the future.apply and doFuture packages three times per year, and the infrastructure packages globals and parallelly three to six times a year. The policy is to fix bugs as soon as possible and avoid breaking updates to maintain full backward compatibility with existing R code in production that rely on futures. In the very rare case when an existing feature had to be removed, it has been done via deprecation process taking place over several release cycles working closely with existing package developers to assure a smooth transition and with informative warning messages to end-users where needed."
  },
  {
    "objectID": "statistics.html",
    "href": "statistics.html",
    "title": "Real-world Use & Statistics",
    "section": "",
    "text": "This page has been merged into https://www.futureverse.org/usage.html."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Futureverse",
    "section": "",
    "text": "The future framework makes it easy to parallelize existing R code - often with only a minor change of code. The goal is to lower the barriers so that anyone can safely speed up their existing R code in a worry-free manner. As it is a cross-platform solution that requires no additional setups or technical skills, anyone can be up and running within a few minutes.\nThe future framework removes common hurdles and protects against pitfalls that follow from adding parallelization. Instead of leaving it to the developers and end-users to be aware of and deal with these problems, they are handled at the core of the highly-validated future ecosystem. Just as with sequential R code, output, messages, warnings, and errors work as expected and can be handled using traditional R techniques - regardless how the code is parallelized.\nIt is designed so that you as a developer can stay with your favorite coding style, may it be base R or tidyverse. If you like base R lapply() there is a corresponding future_lapply() in the future.apply package and if you like tidyverse purrr map() there is a corresponding future_map() in the furrr package. If you prefer foreach() from foreach, then you can do so via doFuture.\nFutures makes your web interface asynchronous, e.g. a blocking Shiny application can easily be turned into a non-blocking experience by using futures.\nRegardless how you use futures in your code, the user can, with a single setting, switch from running your code sequentially to running it in parallel on their local computer, across multiple machines on their local area network, in the cloud, or distributed on a high-performance compute (HPC) cluster.\nFor further details and motivations, see Bengtsson (2021).\n\nHere’s the gist of how to use it:\nlibrary(future)\nplan(multisession)\n\n## Evaluate an R expression sequentially\ny &lt;- slow_fcn(X[1])\n\n## Evaluate it in parallel in the background\nf &lt;- future(slow_fcn(X[1]))\ny &lt;- value(f)\n\n## future.apply: futurized version of base R apply\nlibrary(future.apply)\ny &lt;-        lapply(X, slow_fcn)\ny &lt;- future_lapply(X, slow_fcn)\n\n## furrr: futurized version of purrr\nlibrary(furrr)\ny &lt;- X |&gt;        map(slow_fcn)\ny &lt;- X |&gt; future_map(slow_fcn)\n\n## foreach: futurized version (modern)\nlibrary(foreach)\ny &lt;- foreach(x = X) %do%       slow_fcn(x)\ny &lt;- foreach(x = X) %dofuture% slow_fcn(x)\n\n## foreach: futurized version (traditional)\nlibrary(foreach)\ndoFuture::registerDoFuture()\ny &lt;- foreach(x = X) %do%    slow_fcn(x)\ny &lt;- foreach(x = X) %dopar% slow_fcn(x)\n\n\n\n\n\nReferences\n\nBengtsson, Henrik. 2021. “A Unifying Framework for Parallel and Distributed Processing in R using Futures.” The R Journal. https://journal.r-project.org/archive/2021/RJ-2021-048/index.html."
  },
  {
    "objectID": "tutorials.html",
    "href": "tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "Futureverse: A Unifying Parallelization Framework in R for Everyone (RaukR 2025)\n      \n    \n    \n      Henrik Bengtsson\n      RaukR Summer School 2025, R Beyond the Basics, Visby, Sweden\n      2025-06-12\n    \n  \n\n\n\n\n\n\n  \n    \n      \n        Futureverse: Friendly Parallelization in R (useR! 2024)\n      \n    \n    \n      Henrik Bengtsson\n      useR! 2024, Salzburg, Austria\n      2024-07-08\n    \n  \n\n\n\n\n\n\n  \n    \n      \n        Futureverse: A Unifying Parallelization Framework in R for Everyone (RaukR 2024)\n      \n    \n    \n      Henrik Bengtsson\n      RaukR Summer School 2024, Advanced R for Bioinformatics, Visby, Sweden\n      2024-06-12 and 2024-06-17\n    \n  \n\n\n\n\n\n\n  \n    \n      \n        Futureverse - A Unifying Parallelization Framework in R for Everyone: STATS/BIODS 352: Topics in Computing for Data Science, Bridging Methodology and Practice, Stanford University, 2023\n      \n    \n    \n      Henrik Bengtsson\n      Stanford University, California, United States\n      2023-04-20 and 2023-04-27\n    \n  \n\n\n\n\n\n\n  \n    \n      \n        An Introduction to Futureverse for Parallel Processing in R (useR! 2022)\n      \n    \n    \n      Henrik Bengtsson\n      useR! 2022 conference, worldwide (virtual)\n      2022-06-20"
  },
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Real-World Use & Statistics",
    "section": "",
    "text": "If we look at our two main R package repositories, CRAN and Bioconductor, we find that the future framework is used by R packages spanning a wide range of areas, e.g. statistics, modeling & prediction, time-series analysis & forecasting, life sciences, drug analysis, clinical trials, disease modeling, cancer research, computational biology, genomics, bioinformatics, biomarker discovery, epidemiology, ecology, economics & finance, spatial, geospatial & satellite analysis, and natural language processing. That is just a sample based on published R packages - we can only guess how futures are used by users at the R prompt, in users’ R scripts, non-published R packages, Shiny applications, and R pipelines running internally in the industry and academia.\nThere are two major use cases of the future framework: (i) performance improvement through parallelization, and (ii) non-blocking, asynchronous user experience (UX). Below are some prominent examples. More examples can be found on R-university, which lists ~1,400 CRAN packages that need the future package.\n\n\n\n\n\n\n\n\nEpiNow2 is an R package to estimate real-time case counts and time-varying epidemiological parameters, such as current trends of COVID-19 incidents in different regions around the globe.\nEpiNow2 uses futures to speed up processing. The future framework is used to estimate incident rates in different regions concurrently as well as running Markov Chain Monte Carlo (MCMC) in parallel.\n\n\n\n\n\n\n\n\n\nSeurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. Azimuth is a Seurat-based web application, e.g. HuBMAP - NIH Human Biomolecular Atlas Project\nSeurat uses futures to speed up processing. The future framework makes it possible to process large data sets and large number of samples in parallel on the local machine, distributed on multiple machines, or via large-scale high-performance compute (HPC) environments. Azimuth uses futures to provide a non-blocking web interface.\n\n\n\n\n\n\n\n\n\nShiny is an R package that makes it easy to build interactive web applications and dashboards directly from R. Shiny apps can run locally, be embedded in an R Markdown document, and be hosted on a webpage - all with a few clicks or commands. The combination of being simple and powerful has made Shiny the most popular solution for web applications in the R community. See the Shiny Gallery for real-world examples, e.g. the Genome Browser by the International Cancer Genome Consortium (ICGC) team.\nShiny uses the future framework to provide a non-blocking user interface and to scale up computationally heavy requests. It combines future with promises to turn a blocking, synchronous web interface into a non-blocking, asynchronous, responsive user experience.\n\n\n\n\n\n\n\n\n\nThe mlr3 ecosystem provides efficient, object-oriented building blocks for machine learning (ML) for tasks, learners, resamplings, and measures. It supports large-scale, out-of-memory data processing.\nmlr3 uses futures to speed up processing. The future framework is used in different ML steps, e.g. resampling of learners can be performed much faster when run in parallel. The framework makes sure proper parallel random-number generation (RNG) is used and guarantees reproducible results.\n\n\n\n\n\n\n\n\n\nThe targets package, and its predecessor drake, is a general-purpose computational engine for statistics and data science that brings together function-oriented programming in R with make-like declarative workflows. It has native support for parallel and distributed computing while preserving reproducibility.\nBoth targets and drake identify targets in the declared dependency graph that can be resolved concurrently, which then can be processed in parallel on the local computer or distributed in the cloud via the future framework."
  },
  {
    "objectID": "usage.html#epinow2-estimate-real-time-case-counts-and-time-varying-epidemiological-parameters",
    "href": "usage.html#epinow2-estimate-real-time-case-counts-and-time-varying-epidemiological-parameters",
    "title": "Real-World Use & Statistics",
    "section": "",
    "text": "EpiNow2 is an R package to estimate real-time case counts and time-varying epidemiological parameters, such as current trends of COVID-19 incidents in different regions around the globe.\nEpiNow2 uses futures to speed up processing. The future framework is used to estimate incident rates in different regions concurrently as well as running Markov Chain Monte Carlo (MCMC) in parallel."
  },
  {
    "objectID": "usage.html#seurat-large-scale-single-cell-genomics",
    "href": "usage.html#seurat-large-scale-single-cell-genomics",
    "title": "Real-World Use & Statistics",
    "section": "",
    "text": "Seurat is an R package designed for QC, analysis, and exploration of single-cell RNA-seq data. Seurat aims to enable users to identify and interpret sources of heterogeneity from single-cell transcriptomic measurements, and to integrate diverse types of single-cell data. Azimuth is a Seurat-based web application, e.g. HuBMAP - NIH Human Biomolecular Atlas Project\nSeurat uses futures to speed up processing. The future framework makes it possible to process large data sets and large number of samples in parallel on the local machine, distributed on multiple machines, or via large-scale high-performance compute (HPC) environments. Azimuth uses futures to provide a non-blocking web interface."
  },
  {
    "objectID": "usage.html#shiny-scalable-asynchronous-ux",
    "href": "usage.html#shiny-scalable-asynchronous-ux",
    "title": "Real-World Use & Statistics",
    "section": "",
    "text": "Shiny is an R package that makes it easy to build interactive web applications and dashboards directly from R. Shiny apps can run locally, be embedded in an R Markdown document, and be hosted on a webpage - all with a few clicks or commands. The combination of being simple and powerful has made Shiny the most popular solution for web applications in the R community. See the Shiny Gallery for real-world examples, e.g. the Genome Browser by the International Cancer Genome Consortium (ICGC) team.\nShiny uses the future framework to provide a non-blocking user interface and to scale up computationally heavy requests. It combines future with promises to turn a blocking, synchronous web interface into a non-blocking, asynchronous, responsive user experience."
  },
  {
    "objectID": "usage.html#mlr3-next-generation-machine-learning",
    "href": "usage.html#mlr3-next-generation-machine-learning",
    "title": "Real-World Use & Statistics",
    "section": "",
    "text": "The mlr3 ecosystem provides efficient, object-oriented building blocks for machine learning (ML) for tasks, learners, resamplings, and measures. It supports large-scale, out-of-memory data processing.\nmlr3 uses futures to speed up processing. The future framework is used in different ML steps, e.g. resampling of learners can be performed much faster when run in parallel. The framework makes sure proper parallel random-number generation (RNG) is used and guarantees reproducible results."
  },
  {
    "objectID": "usage.html#targets-pipeline-toolkit-for-reproducible-computation-at-scale",
    "href": "usage.html#targets-pipeline-toolkit-for-reproducible-computation-at-scale",
    "title": "Real-World Use & Statistics",
    "section": "",
    "text": "The targets package, and its predecessor drake, is a general-purpose computational engine for statistics and data science that brings together function-oriented programming in R with make-like declarative workflows. It has native support for parallel and distributed computing while preserving reproducibility.\nBoth targets and drake identify targets in the declared dependency graph that can be resolved concurrently, which then can be processed in parallel on the local computer or distributed in the cloud via the future framework."
  },
  {
    "objectID": "usage.html#footnotes",
    "href": "usage.html#footnotes",
    "title": "Real-World Use & Statistics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nImportantly, the comparison toward foreach is only done as a reference for the current demand for parallelization frameworks in R and to show the rapid uptake of the future framework since its release. It is not a competition because foreach can per design be used in companion with the future framework via doFuture. The choice between foreach with doFuture, future.apply, and furrr is a matter of preference of coding style - they all rely on futures for parallelization.↩︎\nBecause historical data for reverse dependencies on Bioconductor are hard to track down, Bioconductor packages are not included in these graphs.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "The project lead, Henrik Bengtsson, has a MSc in Computer Science and a PhD in Mathematical Statistics. He is an associate professor at the Department of Epidemiology & Biostatistics, at University of California San Francisco (UCSF) and affiliated with the UCSF Helen Diller Family Comprehensive Cancer Center. Henrik has more than a decade of experience in supporting and managing large-scale high-performance compute (HPC) environments used by thousands of users.\nHenrik is one of forty members of the R Foundation - a non-profit organization that supports the R Project, its continued development of the R language, and the R community around it. Through this work, he is also the director of the R Consortium Infrastructure Software Committee (ISC), which supports research & development activities related to R through a grant program with financial support from leading industry investors.\nHe has developed small and large-scale scientific software for 25 years within areas of statistics, bioinformatics, genomics, cancer research, and computational sciences. These tools have been developed as part of real-world collaborative research projects going from scientific question, to statistical models and estimators, and implementation and validation. It is throughout the computational needs of these small and large-scale projects that the seed to what would become the future ecosystem sprung out.\nHenrik has worked with R since 2000 and has contributed with many R packages on CRAN and Bioconductor. With more than 35 CRAN packages, he is among the top-10 maintainers there. Several of his packages are among the top-1% most downloaded on CRAN. He has also developed and contributed to several Bioconductor packages. He has also contributed with design and implementation to the R core distribution.\nOpen access, open source, cross-platform operability, long-term stability, correctness, and reproducibility are absolute top priorities throughout all his software projects."
  },
  {
    "objectID": "about.html#about-the-project-lead",
    "href": "about.html#about-the-project-lead",
    "title": "About",
    "section": "",
    "text": "The project lead, Henrik Bengtsson, has a MSc in Computer Science and a PhD in Mathematical Statistics. He is an associate professor at the Department of Epidemiology & Biostatistics, at University of California San Francisco (UCSF) and affiliated with the UCSF Helen Diller Family Comprehensive Cancer Center. Henrik has more than a decade of experience in supporting and managing large-scale high-performance compute (HPC) environments used by thousands of users.\nHenrik is one of forty members of the R Foundation - a non-profit organization that supports the R Project, its continued development of the R language, and the R community around it. Through this work, he is also the director of the R Consortium Infrastructure Software Committee (ISC), which supports research & development activities related to R through a grant program with financial support from leading industry investors.\nHe has developed small and large-scale scientific software for 25 years within areas of statistics, bioinformatics, genomics, cancer research, and computational sciences. These tools have been developed as part of real-world collaborative research projects going from scientific question, to statistical models and estimators, and implementation and validation. It is throughout the computational needs of these small and large-scale projects that the seed to what would become the future ecosystem sprung out.\nHenrik has worked with R since 2000 and has contributed with many R packages on CRAN and Bioconductor. With more than 35 CRAN packages, he is among the top-10 maintainers there. Several of his packages are among the top-1% most downloaded on CRAN. He has also developed and contributed to several Bioconductor packages. He has also contributed with design and implementation to the R core distribution.\nOpen access, open source, cross-platform operability, long-term stability, correctness, and reproducibility are absolute top priorities throughout all his software projects."
  },
  {
    "objectID": "backends.html",
    "href": "backends.html",
    "title": "Parallel Backends",
    "section": "",
    "text": "By default, future-based code runs sequentially, but with a single line of code, we easily switch to run the exact same code in parallel. The most common approach is to parallelize on the local machine, but we have also the option to harness the CPUs of other local or remote machines. For example, to parallelize on the local machine, the end-user can call:\nplan(multisession)\nAfter this, all of Futureverse, including future.apply, furrr, and doFuture, and any package that use these, will run the code in parallel.\nTo switch back to sequential processing, we can call:\nplan(sequential)\nIf you have Secure Shell (SSH) access to other machines on your local network, or remote machines, call:\nplan(cluster, workers = c(\"n1\", \"n1\", \"n2\", \"remote.server.org\"))\nThis will set up four parallel workers, where two run on the local ‘n1’ machine, another on the local ‘n2’ machine, and the fourth on the remote ‘remote.server.org’ machine.\nThe future package comes with built-in future backends that leverage the parallel package part of R itself. In addition to these backends, others exist in package extensions, e.g. future.callr, future.mirai, and future.batchtools. Below is an overview of the most common backends that you as an end-user can chose from.\n\n\n\nPackage / Backend\nFeatures\nHow futures are evaluated\n\n\n\n\nfuture sequential\n📶♻️\nsequentially and in the current R process; defaultExample: plan(sequential)\n\n\nfuture multisession\n📶♻️\nparallelly via background R sessions on current machineExamples: plan(multisession) and plan(multisession, workers = 2)\n\n\nfuture cluster\n📶♻️*\nparallelly in external R sessions on current, local, and/or remote machinesExamples: plan(cluster, workers = \"raspberry-pi\"), plan(cluster, workers = c(\"localhost\", \"n1\", \"n1\", \"pi.example.org\"))\n\n\nfuture multicore\n📶♻️\n(not recommended) parallelly via forked R processes on current machine; not with GUIs like RStudio; not on WindowsExamples: plan(multicore) and plan(multicore, workers = 2)\n\n\nfuture.callr callr\n📶♻️\nparallelly via transient callr background R sessions on current machine; all memory is returned when as each future is resolvedExamples: plan(callr) and plan(callr, workers = 2)\n\n\nfuture.mirai mirai_multisession\n📶♻️\nparallelly via mirai background R sessions on current machine; low latencyExamples: plan(mirai_multisession) and plan(mirai_multisession, workers = 2)\n\n\nfuture.mirai mirai_cluster\n♻️\nparallelly via mirai daemons running locally or remotelyExample: plan(mirai_cluster)\n\n\nfuture.batchtools batchtools_lsfbatchtools_openlavabatchtools_sgebatchtools_slurmbatchtools_torque\n📶(soon) ♻️(next)\nparallelly on HPC job schedulers (Load Sharing Facility [LSF], OpenLava, TORQUE/PBS, Son/Sun/Oracle/Univa Grid Engine [SGE], Slurm) via batchtools; for long-running tasks; high latency\n\n\n\n📶: futures relay progress updates in real-time, e.g. progressr ♻️: futures are interruptible and restartable; * disabled by default (soon): in a near-future release\nIt is straightforward to implement new backends that leverage other ways to harness available compute resources. As soon as a new backend has been validated to be compliant with the Future API specifications, which can be done by the future.tests package, then it can be used anywhere future-based code is used."
  },
  {
    "objectID": "roadmap.html",
    "href": "roadmap.html",
    "title": "Project Roadmap",
    "section": "",
    "text": "Below are some1 of the bigger milestones on the roadmap. The issues linked that are examples of feature requests from end-users are prefixed ‘FR’, which will be resolved by the other non-FR issues."
  },
  {
    "objectID": "roadmap.html#core-future-api",
    "href": "roadmap.html#core-future-api",
    "title": "Project Roadmap",
    "section": "Core Future API",
    "text": "Core Future API\n\nRestarting of failed or terminated futures, e.g. reset(f) and v &lt;- value(f). Also automatic restarting of futures when parallel worker dies [future#FR188, future#205, future.callr#FR11, parallelly#32] (solved in future 1.40.0; future backends will be updated accordingly)\nCancellation of futures with interruption, i.e. cancel(f). All futures can be canceled, but not all can be interrupted. If a future backend does not support interrupting futures, then the interrupt requires is silently ignored. [future#93, future#FR213, future.batchtools#FR27, parallelly#33] (solved in future 1.49.0; future backends will be updated accordingly)\nPerformance: Add built-in speed and memory profiling tools to help developers and users identify bottle necks and improve overall performance [future#59, future#142, future#FR437]\nTroubleshooting: Improve debugging of future that produce errors, e.g. capture call frames in parallel workers and let user inspect them via tools such as utils::browser() and utils::recover() [future#253]\nSupport for controlling R options and environment variables via future() [future#480]\nMarshaling: Make it possible to parallelize also some of the object types that are currently “non-exportable”, e.g. objects by packages such as DBI, keras, ncdf4, rstan, sparklyr, xgboost, and xml2 [Project: marshal and R Consortium ISC Marshaling and Serialization in R Working Group]\nResources: Make it possible to declare “resources” that a future requires in order to be resolved, e.g. minimal memory requirements, access to the local file system, a specific operating system, internet access, sandboxed, etc. This will also make it possible to work with semi-persistent “sticky” globals [doFuture#FR63, future#FR181, future#FR301, future#FR346, future#FR430, future#FR450, parallelly#18]\nMultiple backends: Send different futures to different backends, based on which parallel backend can provide the requested resources\nScheduling: Add generic support for queueing such that futures can be queued on the local computer, on a remote scheduler, or in peer-to-peer scheduler among collaborators [future#FR256, future.apply#FR63, future.batchtools#FR23]\nFlattening nested parallelism: Process nested futures via a common future queue instead of via nested future plans [future#FR361]\nRandom number generation: Support for custom, alternative RNGs other than the built-in L’Ecuyer-CMRG algorithm."
  },
  {
    "objectID": "roadmap.html#map-reduce-future-api",
    "href": "roadmap.html#map-reduce-future-api",
    "title": "Project Roadmap",
    "section": "Map-Reduce Future API",
    "text": "Map-Reduce Future API\n\nDevelop a future.mapreduce package to provide an essential, infrastructure Map-Reduce Future API, e.g. load balancing, chunking, parallel random number generation (RNG), and early stopping. Packages such as future.apply, furrr, and doFuture are currently implementing their own version of these features. Consolidating these essentials into a shared utility package will guarantee a consistent behavior for all together with faster deployment of bug fixes and improvements [future.apply#20, future.apply#FR32, future.apply#FR44, future.apply#59, future.apply#FR60]\nDevelop more efficient chunking of large objects in map-reduce calls. This can be achieved by introducing a mechanism or a syntax to declare that part of the code should be processed prior to being parallelized, e.g. further subsetting of and validation of input data before distributing them to parallel workers\nEarly stopping, e.g. have future_lapply(X, FUN) terminate active futures as soon as one of the FUN(X[[i]]) calls produces an error [future#FR213, future.apply#FR75]\nAutomatic map-reduce: Support for merging of many futures into chunks of futures to be processed more efficiently with less total overhead. This could make fs &lt;- lapply(X, function(x) future(FUN(x)) and vs &lt;- value(fs) automatically as efficient as future_lapply(X, FUN) removing the need for custom implementations\nRobustness of foreach: Implement bug fixes and outstanding foreach feature requests in the doFuture adapter until resolved upstream. For example, add withDoRNG() and %dofuture%. [doFuture#61] (solved in doFuture 0.13.0 and doFuture 1.0.0)\nGeneric support for progress updates for common map-reduce functions, e.g. y &lt;- lapply(X, slow_fcn) %progress% TRUE and y &lt;- X |&gt; future_map(slow_fcn) %progress% TRUE [progressr#FR85, progressr#113]"
  },
  {
    "objectID": "roadmap.html#parallel-backends",
    "href": "roadmap.html#parallel-backends",
    "title": "Project Roadmap",
    "section": "Parallel backends",
    "text": "Parallel backends\nBelow is a list of future backends that are worked on or in the plans:\n\nfuture.clustermq - a backend to resolve futures on a HPC cluster via the clustermq package [future#FR204, future#FR267]\nfuture.redis - a backend to resolve futures via a Redis queue using the redux package [future#FR151]\nfuture.rrq - a backend to resolve futures via a Redis queue using the rrq package [future#FR151]\nfuture.aws.lambda - a backend to resolve futures via the Amazon AWS Lambda service. A group of several people is actively working on this since the end of 2020 [future#FR423]\nfuture.aws.batch - a backend to resolve futures via the Amazon AWS Batch service. We have working group of several people actively working on this [future#FR423]\nfuture.google.cloud.functions - a backend to resolve futures via the Google Cloud Functions service. Work on this will start when a stable future.aws.lambda prototype has been established\nfuture.p2p - a backend to resolve futures via a peer-to-peer network of trusted collaborators [available as of 2025-08-10]\nfuture.sandbox - a backend to resolve untrusted futures via locked-down, sandboxed Linux containers (Docker, Singularity, …) without access to the hosts file system or network. This can already be partially achieved by low-level container setups via parallelly\nfuture.sparklyr - a backend to resolve futures in Spark via the sparklyr package [future#FR286, sparklyr#FR1935]"
  },
  {
    "objectID": "roadmap.html#miscellaneous",
    "href": "roadmap.html#miscellaneous",
    "title": "Project Roadmap",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nInternationalization (i18n): Make all error and warning messages translatable via R’s gettext() framework. Invite community to provide message translations.\nConsider relicensing code to a permissive license, where possible"
  },
  {
    "objectID": "roadmap.html#footnotes",
    "href": "roadmap.html#footnotes",
    "title": "Project Roadmap",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis list is manual curated and may not be comprehensive. Please see the issue trackers and milestones of the different package repositories for a more up-to-date view, e.g. https://github.com/futureverse/future/milestones.↩︎"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog Posts",
    "section": "",
    "text": "useR! 2025: Futureverse P2P: Peer-to-Peer Parallelization in R - Share compute among friends across the world\n\n\n\n\nHenrik Bengtsson\n\n\n2025-08-14\n\n\n\n\n\n\n\n\n\n\n\nSetting Future Plans in R Functions — and Why You Probably Shouldn’t\n\n\n\n\nHenrik Bengtsson\n\n\n2025-06-25\n\n\n\n\n\n\n\n\n\n\n\nFuture Got Better at Finding Global Variables\n\n\n\n\nHenrik Bengtsson\n\n\n2025-06-23\n\n\n\n\n\n\n\n\n\n\n\nFutureverse – Ten-Year Anniversary\n\n\n\n\nHenrik Bengtsson\n\n\n2025-06-19\n\n\n\n\n\n\n\n\n\n\n\nparallelly: Querying, Killing and Cloning Parallel Workers Running Locally or Remotely\n\n\n\n\nHenrik Bengtsson\n\n\n2023-07-01\n\n\n\n\n\n\n\n\n\n\n\n%dofuture% - a Better foreach() Parallelization Operator than %dopar%\n\n\n\n\nHenrik Bengtsson\n\n\n2023-06-26\n\n\n\n\n\n\n\n\n\n\n\nparallelly 1.34.0: Support for CGroups v2, Killing Parallel Workers, and more\n\n\n\n\nHenrik Bengtsson\n\n\n2023-01-18\n\n\n\n\n\n\n\n\n\n\n\nprogressr 0.13.0: cli + progressr = ♥\n\n\n\n\nHenrik Bengtsson\n\n\n2023-01-10\n\n\n\n\n\n\n\n\n\n\n\nPlease Avoid detectCores() in your R Packages\n\n\n\n\nHenrik Bengtsson\n\n\n2022-12-05\n\n\n\n\n\n\n\n\n\n\n\nuseR! 2022: My ‘Futureverse: Profile Parallel Code’ Slides\n\n\n\n\nHenrik Bengtsson\n\n\n2022-06-24\n\n\n\n\n\n\n\n\n\n\n\nparallelly: Support for Fujitsu Technical Computing Suite High-Performance Compute (HPC) Environments\n\n\n\n\nHenrik Bengtsson\n\n\n2022-06-09\n\n\n\n\n\n\n\n\n\n\n\nparallelly 1.32.0: makeClusterPSOCK() Didn’t Work with Chinese and Korean Locales\n\n\n\n\nHenrik Bengtsson\n\n\n2022-06-08\n\n\n\n\n\n\n\n\n\n\n\nprogressr 0.10.1: Plyr Now Supports Progress Updates also in Parallel\n\n\n\n\nHenrik Bengtsson\n\n\n2022-06-03\n\n\n\n\n\n\n\n\n\n\n\nparallelly 1.31.1: Better at Inferring Number of CPU Cores with Cgroups and Linux Containers\n\n\n\n\nHenrik Bengtsson\n\n\n2022-04-22\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.24.0: Forwarding RNG State also for Stand-Alone Futures\n\n\n\n\nHenrik Bengtsson\n\n\n2022-02-22\n\n\n\n\n\n\n\n\n\n\n\nFuture Improvements During 2021\n\n\n\n\nHenrik Bengtsson\n\n\n2022-01-07\n\n\n\n\n\n\n\n\n\n\n\nparallelly 1.29.0: New Skills and Less Communication Latency on Linux\n\n\n\n\nHenrik Bengtsson\n\n\n2021-11-22\n\n\n\n\n\n\n\n\n\n\n\nprogressr 0.8.0: RStudio’s Progress Bar, Shiny Progress Updates, and Absolute Progress\n\n\n\n\nHenrik Bengtsson\n\n\n2021-06-11\n\n\n\n\n\n\n\n\n\n\n\nparallelly 1.26.0: Fast, Concurrent Setup of Parallel Workers (Finally)\n\n\n\n\nHenrik Bengtsson\n\n\n2021-06-10\n\n\n\n\n\n\n\n\n\n\n\nparallelly 1.25.0: availableCores(omit=n) and, Finally, Built-in SSH Support for MS Windows 10 Users\n\n\n\n\nHenrik Bengtsson\n\n\n2021-04-30\n\n\n\n\n\n\n\n\n\n\n\nUsing Kubernetes and the Future Package to Easily Parallelize R in the Cloud\n\n\n\n\nChris Paciorek\n\n\nA guest post by Chris Paciorek, Department of Statistics, University of California at Berkeley.\n\n\n2021-04-08\n\n\n\n\n\n\n\n\n\n\n\nfuture.BatchJobs - End-of-Life Announcement\n\n\n\n\nHenrik Bengtsson\n\n\n2021-01-08\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.20.1 - The Future Just Got a Bit Brighter\n\n\n\n\nHenrik Bengtsson\n\n\n2020-11-06\n\n\n\n\n\n\n\n\n\n\n\nparallelly, future - Cleaning Up Around the House\n\n\n\n\nHenrik Bengtsson\n\n\n2020-11-04\n\n\n\n\n\n\n\n\n\n\n\nTrust the Future\n\n\n\n\nHenrik Bengtsson\n\n\n2020-11-04\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.19.1 - Making Sure Proper Random Numbers are Produced in Parallel Processing\n\n\n\n\nHenrik Bengtsson\n\n\n2020-09-22\n\n\n\n\n\n\n\n\n\n\n\nDetect When the Random Number Generator Was Used\n\n\n\n\nHenrik Bengtsson\n\n\n2020-09-21\n\n\n\n\n\n\n\n\n\n\n\nfuture and future.apply - Some Recent Improvements\n\n\n\n\nHenrik Bengtsson\n\n\n2020-07-11\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.15.0 - Lazy Futures are Now Launched if Queried\n\n\n\n\nHenrik Bengtsson\n\n\n2019-11-09\n\n\n\n\n\n\n\n\n\n\n\nParallelize a For-Loop by Rewriting it as an Lapply Call\n\n\n\n\nHenrik Bengtsson\n\n\n2019-01-11\n\n\n\n\n\n\n\n\n\n\n\nMaintenance Updates of Future Backends and doFuture\n\n\n\n\nHenrik Bengtsson\n\n\n2019-01-07\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.9.0 - Output from The Future\n\n\n\n\nHenrik Bengtsson\n\n\n2018-07-23\n\n\n\n\n\n\n\n\n\n\n\nfuture.apply - Parallelize Any Base R Apply Function\n\n\n\n\nHenrik Bengtsson\n\n\n2018-06-23\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.8.0: Preparing for a Shiny Future\n\n\n\n\nHenrik Bengtsson\n\n\n2018-04-12\n\n\n\n\n\n\n\n\n\n\n\nThe Many-Faced Future\n\n\n\n\nHenrik Bengtsson\n\n\n2017-06-05\n\n\n\n\n\n\n\n\n\n\n\ndoFuture: A Universal Foreach Adaptor Ready to be Used by 1,000+ Packages\n\n\n\n\nHenrik Bengtsson\n\n\n2017-03-18\n\n\n\n\n\n\n\n\n\n\n\nfuture 1.3.0: Reproducible RNGs, future_lapply() and More\n\n\n\n\nHenrik Bengtsson\n\n\n2017-02-19\n\n\n\n\n\n\n\n\n\n\n\nHigh-Performance Compute in R Using Futures \n\n\n\n\nHenrik Bengtsson\n\n\n2016-10-22\n\n\n\n\n\n\n\n\n\n\n\nRemote Processing Using Futures\n\n\n\n\nHenrik Bengtsson\n\n\n2016-10-11"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "H. Bengtsson, A Unifying Framework for Parallel and Distributed Processing in R using Futures, The R Journal (2021) 13:2, pages 208-227 [abstract, PDF]\nH. Bengtsson. Futureverse - Worry-Free Parallelization in R, Biometric Bulletin (2023), 40:3: pages 8–10 [PDF]"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Futureverse P2P: Peer-to-Peer Parallelization in R - Share compute among friends across the world\n    \n    \n      Henrik Bengtsson\n      useR! 2025, Durham, NC, USA\n      2025-08-10 (18 minutes)\n    \n    HTML\n    PDF\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      future.mirai: Use the Mirai Parallelization Framework in Futureverse - Easy!\n    \n    \n      Henrik Bengtsson\n      useR! 2024, Salzburg, Austria\n      2024-07-09 (20 minutes)\n    \n    HTML\n    PDF\n  \n\n\n\n\n\n\n  \n    \n      Futureverse - A Unifying Parallelization Framework in R for Everyone\n    \n    \n      Henrik Bengtsson\n      Edmonton R User Group Meetup (YEGRUG) (virtual)\n      2023-05-22 (60 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Futureverse: Profile Parallel Code\n    \n    \n      Henrik Bengtsson\n      useR! 2022, worldwide (virtual)\n      2022-06-22 (25 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future - Simple, Scalable Parallelization in R for the Biomedical Community\n    \n    \n      Henrik Bengtsson\n      CZI EOSS 2021, worldwide (virtual)\n      2021-11-02 (10 minutes)\n    \n    HTML\n  \n\n\n\n\n\n\n  \n    \n      Future: A Simple, Extendable, Generic Framework for Parallel Processing in R\n    \n    \n      Henrik Bengtsson\n      European Bioconductor Meeting 2020, worldwide (virtual)\n      2020-12-19 (35 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future: Simple, Friendly Parallel Processing for R\n    \n    \n      Henrik Bengtsson\n      New York Open Statistical Programming Meetup, worldwide (virtual)\n      2020-11-12 (67 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Progressr: An Inclusive, Unifying API for Progress Updates\n    \n    \n      Henrik Bengtsson\n      e-Rum 2020, worldwide (virtual)\n      2020-06-17 (15 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future: Simple Async, Parallel & Distributed Processing in R Why and What’s New?\n    \n    \n      Henrik Bengtsson\n      rstudio::conf 2020, San Francisco, California, USA\n      2020-01-29 (18 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future: Simple Parallel and Distributed Processing in R\n    \n    \n      Henrik Bengtsson\n      useR! 2019, Toulouse, France\n      2019-07-12 (15 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future: Friendly Parallel Processing in R for Everyone\n    \n    \n      Henrik Bengtsson\n      satRday LA 2019, Los Angeles, USA\n      2019-04-06 (45 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future: Friendly Parallel Processing in R for Everyone\n    \n    \n      Henrik Bengtsson\n      satRday Paris 2019, Paris, France\n      2019-02-23 (50 minutes)\n    \n    HTML\n    PDF\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Future: Parallel & Distributed Processing in R for Everyone\n    \n    \n      Henrik Bengtsson\n      eRum 2018, Budapest, Hungary\n      2018-05-16 (20 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post\n  \n\n\n\n\n\n\n  \n    \n      Futures in R: Atomic Building Blocks for Asynchronous Evaluation\n    \n    \n      Henrik Bengtsson\n      R Consortium, Distributed Computing Working Group, worldwide (virtual)\n      2017-05-11 (60 minutes)\n    \n    PDF\n    Event\n  \n\n\n\n\n\n\n  \n    \n      A Future for R\n    \n    \n      Henrik Bengtsson\n      useR! 2016, Stanford, California, USA\n      2016-06-28 (18 minutes)\n    \n    HTML\n    PDF\n    Video\n    Blog post"
  },
  {
    "objectID": "packages-overview.html",
    "href": "packages-overview.html",
    "title": "Overview of All Packages",
    "section": "",
    "text": "The core package\n\nfuture - Unified Parallel and Distributed Processing in R for Everyone This is the core package of the future framework. It implements the Future API, which comprises three basic functions - future(), resolved(), and value(), is designed to unify parallel processing in R at the lowest possible level. It provides a standard for building richer, higher-level parallel frontends without having to worry about and re-implement common, critical tasks such as identifying global variables and packages, parallel RNG, and relaying of output and conditions - cumbersome tasks that are often essential to parallel processing. Nearly all parallel processing tasks can be implemented using this low-level API. But, most users and package developers use the futures via higher-level map-reduce functions provided by future.apply, furrr, and foreach with doFuture.\n\n\n\nParallel map-reduce functions for popular programming style\n\nfuture.apply - Apply Function to Elements in Parallel using Future Implementations of apply(), by(), eapply(), lapply(), Map(), .mapply(), mapply(), replicate(), sapply(), tapply(), and vapply() that can be resolved using any future-supported backend, e.g. parallel on the local machine or distributed on a compute cluster. These future_*apply() functions come with the same pros and cons as the corresponding base R *apply() functions but with the additional feature of being able to be processed via the future framework.\nfurrr - Apply Mapping Functions in Parallel using Futures Implementations of the family of map() functions from purrr that can be resolved using any future-supported backend, e.g. parallel on the local machine or distributed on a compute cluster. (Developed by Davis Vaughan)\nforeach with doFuture - Use Foreach to Parallelize via the Future Framework The doFuture package combines the best of foreach and future and offers two alternative solution: (i) the traditional foreach(...) %dopar% { ... } solution together via registerDoFuture(), and (ii) the modern foreach(...) %dofuture% { ... } by itself.\nBiocParallel with BiocParallel.FutureParam - Use Futures with BiocParallel A ‘BiocParallelParam’ class for using futures with the BiocParallel framework, e.g. BiocParallel::register(FutureParam()). An alternative, is to use BiocParallel::register(DoParam()) in combination of doFuture.\n\n\n\nChoosing how and where to parallelize\n\nfuture - the future package has a set of built-in parallel backends that build upon the parallel package, e.g. multicore, multisession, and cluster.\nfuture.batchtools - A Future API for Parallel and Distributed Processing using batchtools Implementation of the Future API on top of the batchtools package. This allows you to process futures, as defined by the future package, in parallel out of the box, not only on your local machine or ad-hoc cluster of machines, but also via high-performance compute (HPC) job schedulers such as LSF, OpenLava, Slurm, SGE, and TORQUE/PBS, e.g. y &lt;- future.apply::future_lapply(files, FUN = process).\nfuture.callr - A Future API for Parallel Processing using callr Implementation of the Future API on top of the callr package. This allows you to process futures, as defined by the future package, in parallel out of the box, on your local (Linux, macOS, Windows, …) machine. Contrary to backends relying on the parallel package (e.g. future::multisession), the callr backend provided here can run more than 125 parallel R processes.\nfuture.mirai - A Future API for Parallel Processing using mirai Implementation of the Future API on top of the mirai package. This allows you to process futures, as defined by the future package, in parallel out of the box, on local and remote machines. The mirai package implements distributed computing via local or network resources and supports Transport Layer Security over TCP/IP for remote connections.\n\n\n\nReporting on progress\n\nprogressr - An Inclusive, Unifying API for Progress UpdatesA minimal, unifying API for scripts and packages to report progress updates from anywhere including when using parallel processing. The package is designed such that the developer can to focus on what progress should be reported on without having to worry about how to present it. The end user has full control of how, where, and when to render these progress updates, including via progress bars of the popular progress package. The future ecosystem support progressr at its core and many of the backends can report progress in a near-live fashion.\n\n\n\nValidation\n\nfuture.tests - Test Suite for Future API Backends Backends implementing the Future API, as defined by the future package, should use the tests provided by this package to validate that they meet the minimal requirements of the Future API. The tests can be performed easily from within R or from outside of R from the command line making it easy to include them package tests and in Continuous Integration (CI) pipelines.\n\n\n\nSupporting packages\n\nparallelly - Enhancing the parallel Package Utility functions that enhance the parallel package and support the built-in parallel backends of the future package.\nglobals - Identify Global Objects in R Expressions Identifies global (“unknown” or “free”) objects in R expressions by code inspection using various strategies (ordered, liberal, or conservative). The objective of this package is to make it as simple as possible to identify global objects for the purpose of exporting them in parallel, distributed compute environments.\nlistenv - Environments Behaving (Almost) as Lists List environments are environments that have list-like properties. For instance, the elements of a list environment are ordered and can be accessed and iterated over using index subsetting, e.g. x &lt;- listenv(a = 1, b = 2); for (i in seq_along(x)) x[[i]] &lt;- x[[i]] ^ 2; y &lt;- as.list(x).\nmarshal - Framework to Marshal Objects to be Used in Another R Process PROTOTYPE: Some types of R objects can be used only in the R session they were created. If used as-is in another R process, such objects often result in an immediate error or in obscure and hard-to-troubleshoot outcomes. Because of this, they cannot be saved to file and re-used at a later time. They can also not be exported to a worker in parallel processing. These objects are sometimes referred to as non-exportable or non-serializable objects. One solution to this problem is to use “marshaling” to encode the R object into an exportable representation that then can be used to re-create a copy of that object in another R process."
  }
]